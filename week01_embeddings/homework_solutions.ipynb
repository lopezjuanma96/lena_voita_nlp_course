{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework: Multilingual Embedding-based Machine Translation (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this homework** **<font color='red'>YOU</font>** will make machine translation system without using parallel corpora, alignment, attention, 100500 depth super-cool recurrent neural network and all that kind superstuff.\n",
    "\n",
    "But even without parallel corpora this system can be good enough (hopefully). \n",
    "\n",
    "For our system we choose two kindred Slavic languages: Ukrainian and Russian. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feel the difference!\n",
    "\n",
    "(_синій кіт_ vs. _синій кит_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![blue_cat_blue_whale.png](https://github.com/yandexdataschool/nlp_course/raw/master/resources/blue_cat_blue_whale.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frament of the Swadesh list for some slavic languages\n",
    "\n",
    "The Swadesh list is a lexicostatistical stuff. It's named after American linguist Morris Swadesh and contains basic lexis. This list are used to define subgroupings of languages, its relatedness.\n",
    "\n",
    "So we can see some kind of word invariance for different Slavic languages.\n",
    "\n",
    "\n",
    "| Russian         | Belorussian              | Ukrainian               | Polish             | Czech                         | Bulgarian            |\n",
    "|-----------------|--------------------------|-------------------------|--------------------|-------------------------------|-----------------------|\n",
    "| женщина         | жанчына, кабета, баба    | жінка                   | kobieta            | žena                          | жена                  |\n",
    "| мужчина         | мужчына                  | чоловік, мужчина        | mężczyzna          | muž                           | мъж                   |\n",
    "| человек         | чалавек                  | людина, чоловік         | człowiek           | člověk                        | човек                 |\n",
    "| ребёнок, дитя   | дзіця, дзіцёнак, немаўля | дитина, дитя            | dziecko            | dítě                          | дете                  |\n",
    "| жена            | жонка                    | дружина, жінка          | żona               | žena, manželka, choť          | съпруга, жена         |\n",
    "| муж             | муж, гаспадар            | чоловiк, муж            | mąż                | muž, manžel, choť             | съпруг, мъж           |\n",
    "| мать, мама      | маці, матка              | мати, матір, неня, мама | matka              | matka, máma, 'стар.' mateř    | майка                 |\n",
    "| отец, тятя      | бацька, тата             | батько, тато, татусь    | ojciec             | otec                          | баща, татко           |\n",
    "| много           | шмат, багата             | багато                  | wiele              | mnoho, hodně                  | много                 |\n",
    "| несколько       | некалькі, колькі         | декілька, кілька        | kilka              | několik, pár, trocha          | няколко               |\n",
    "| другой, иной    | іншы                     | інший                   | inny               | druhý, jiný                   | друг                  |\n",
    "| зверь, животное | жывёла, звер, істота     | тварина, звір           | zwierzę            | zvíře                         | животно               |\n",
    "| рыба            | рыба                     | риба                    | ryba               | ryba                          | риба                  |\n",
    "| птица           | птушка                   | птах, птиця             | ptak               | pták                          | птица                 |\n",
    "| собака, пёс     | сабака                   | собака, пес             | pies               | pes                           | куче, пес             |\n",
    "| вошь            | вош                      | воша                    | wesz               | veš                           | въшка                 |\n",
    "| змея, гад       | змяя                     | змія, гад               | wąż                | had                           | змия                  |\n",
    "| червь, червяк   | чарвяк                   | хробак, черв'як         | robak              | červ                          | червей                |\n",
    "| дерево          | дрэва                    | дерево                  | drzewo             | strom, dřevo                  | дърво                 |\n",
    "| лес             | лес                      | ліс                     | las                | les                           | гора, лес             |\n",
    "| палка           | кій, палка               | палиця                  | patyk, pręt, pałka | hůl, klacek, prut, kůl, pálka | палка, пръчка, бастун |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the context distribution of these languages demonstrates even more invariance. And we can use this fact for our for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download embeddings here:\n",
    "* [cc.uk.300.vec.zip](https://yadi.sk/d/9CAeNsJiInoyUA)\n",
    "* [cc.ru.300.vec.zip](https://yadi.sk/d/3yG0-M4M8fypeQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load embeddings for ukrainian and russian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_emb = KeyedVectors.load_word2vec_format(\"cc.uk.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_emb = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('август', 0.9999998807907104),\n",
       " ('июль', 0.9383153915405273),\n",
       " ('сентябрь', 0.9240029454231262),\n",
       " ('июнь', 0.9222574830055237),\n",
       " ('октябрь', 0.9095538258552551),\n",
       " ('ноябрь', 0.893003523349762),\n",
       " ('апрель', 0.8729087114334106),\n",
       " ('декабрь', 0.8652557730674744),\n",
       " ('март', 0.8545795679092407),\n",
       " ('февраль', 0.8401415944099426)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_emb.most_similar([ru_emb[\"август\"]], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('серпень', 1.000000238418579),\n",
       " ('липень', 0.9096441268920898),\n",
       " ('вересень', 0.9016969203948975),\n",
       " ('червень', 0.8992519974708557),\n",
       " ('жовтень', 0.8810409307479858),\n",
       " ('листопад', 0.8787633180618286),\n",
       " ('квітень', 0.8592804074287415),\n",
       " ('грудень', 0.8586863279342651),\n",
       " ('травень', 0.840811014175415),\n",
       " ('лютий', 0.8256431221961975)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Недопустимость', 0.24435284733772278),\n",
       " ('конструктивность', 0.23293080925941467),\n",
       " ('офор', 0.23256801068782806),\n",
       " ('deteydlya', 0.23031717538833618),\n",
       " ('пресечении', 0.22632379829883575),\n",
       " ('одностороннего', 0.22608885169029236),\n",
       " ('подход', 0.22305874526500702),\n",
       " ('иболее', 0.22003725171089172),\n",
       " ('2015Александр', 0.21872766315937042),\n",
       " ('конструктивен', 0.21796569228172302)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_emb.most_similar([uk_emb[\"серпень\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load small dictionaries for correspoinding words pairs as trainset and testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_pairs(filename):\n",
    "    uk_ru_pairs = []\n",
    "    uk_vectors = []\n",
    "    ru_vectors = []\n",
    "    total_lines = 0\n",
    "    valid_lines = 0\n",
    "    with open(filename, \"r\") as inpf:\n",
    "        for line in inpf:\n",
    "            total_lines += 1\n",
    "            uk, ru = line.rstrip().split(\"\\t\")\n",
    "            if uk not in uk_emb or ru not in ru_emb:\n",
    "                continue\n",
    "            valid_lines += 1\n",
    "            uk_ru_pairs.append((uk, ru))\n",
    "            uk_vectors.append(uk_emb[uk])\n",
    "            ru_vectors.append(ru_emb[ru])\n",
    "    print(\"{} valid lines out of {}, with a loss of: {:.2f}%\".format(total_lines, valid_lines, 100*(total_lines - valid_lines) / total_lines))\n",
    "    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1927 valid lines out of 1840, with a loss of: 4.51%\n"
     ]
    }
   ],
   "source": [
    "uk_ru_train, X_train, Y_train = load_word_pairs(\"ukr_rus.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 valid lines out of 387, with a loss of: 3.25%\n"
     ]
    }
   ],
   "source": [
    "uk_ru_test, X_test, Y_test = load_word_pairs(\"ukr_rus.test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding space mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_i \\in \\mathrm{R}^d$ be the distributed representation of word $i$ in the source language, and $y_i \\in \\mathrm{R}^d$ is the vector representation of its translation. Our purpose is to learn such linear transform $W$ that minimizes euclidian distance between $Wx_i$ and $y_i$ for some subset of word embeddings. Thus we can formulate so-called Procrustes problem:\n",
    "\n",
    "$$W^*= \\arg\\min_W \\sum_{i=1}^n||Wx_i - y_i||_2$$\n",
    "or\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F$$\n",
    "\n",
    "where $||*||_F$ - Frobenius norm.\n",
    "\n",
    "In Greek mythology, Procrustes or \"the stretcher\" was a rogue smith and bandit from Attica who attacked people by stretching them or cutting off their legs, so as to force them to fit the size of an iron bed. We make same bad things with source embedding space. Our Procrustean bed is target embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![embedding_mapping.png](https://github.com/yandexdataschool/nlp_course/raw/master/resources/embedding_mapping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![procrustes.png](https://github.com/yandexdataschool/nlp_course/raw/master/resources/procrustes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait...$W^*= \\arg\\min_W \\sum_{i=1}^n||Wx_i - y_i||_2$ looks like simple multiple linear regression (without intercept fit). So let's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "mapping = LinearRegression()\n",
    "mapping.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at neigbours of the vector of word _\"серпень\"_ (_\"август\"_ in Russian) after linear transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('апрель', 0.854159414768219),\n",
       " ('июнь', 0.8411962985992432),\n",
       " ('март', 0.8397400379180908),\n",
       " ('сентябрь', 0.8359216451644897),\n",
       " ('февраль', 0.8328749537467957),\n",
       " ('октябрь', 0.8311805725097656),\n",
       " ('ноябрь', 0.8278146982192993),\n",
       " ('июль', 0.8236350417137146),\n",
       " ('август', 0.8120613098144531),\n",
       " ('декабрь', 0.8038000464439392)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n",
    "ru_emb.most_similar(august)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that neighbourhood of this embedding cosists of different months, but right variant is on the ninth place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As quality measure we will use precision top-1, top-5 and top-10 (for each transformed Ukrainian embedding we count how many right target pairs are found in top N nearest neighbours in Russian embedding space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pairs, mapped_vectors, topn=1):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n",
    "        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n",
    "        topn = the number of nearest neighbours in destination embedding space to choose from\n",
    "    :returns:\n",
    "        precision_val, float number, total number of words for those we can find right translation at top K.\n",
    "    \"\"\"\n",
    "    assert len(pairs) == len(mapped_vectors)\n",
    "    num_matches = 0\n",
    "    for i, (_, ru) in enumerate(pairs):\n",
    "        # YOUR CODE HERE\n",
    "        # We check if the right ru word is among the top-n nearest neighbours to the mapped uk word\n",
    "        if ru in [x[0] for x in ru_emb.most_similar([mapped_vectors[i]], topn=topn)]:\n",
    "            num_matches += 1\n",
    "    precision_val = num_matches / len(pairs)\n",
    "    return precision_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n",
    "assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert precision(uk_ru_test, X_test) == 0.0\n",
    "assert precision(uk_ru_test, Y_test) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_top1 = precision(uk_ru_test, mapping.predict(X_test), 1)\n",
    "precision_top5 = precision(uk_ru_test, mapping.predict(X_test), 5)\n",
    "\n",
    "assert precision_top1 >= 0.635, \"Achieved precision: %f\" % precision_top1 + \" not reaching the threshold of 0.635\"\n",
    "assert precision_top5 >= 0.8113, \"Achieved precision: %f\" % precision_top5 + \" not reaching the threshold of 0.813\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it better (orthogonal Procrustean problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown (see original paper) that a self-consistent linear mapping between semantic spaces should be orthogonal. \n",
    "We can restrict transform $W$ to be orthogonal. Then we will solve next problem:\n",
    "\n",
    "$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, where: } W^TW = I$$\n",
    "\n",
    "$$I \\text{- identity matrix}$$\n",
    "\n",
    "Instead of making yet another regression problem we can find optimal orthogonal transformation using singular value decomposition. It turns out that optimal transformation $W^*$ can be expressed via SVD components:\n",
    "$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n",
    "$$W^*=UV^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_transform(X_train, Y_train):\n",
    "    \"\"\"\n",
    "    It can be shown (see original paper) that a self-consistent linear mapping between semantic spaces should be orthogonal. \n",
    "    We can restrict transform W to be orthogonal:  W^T * W = I.\n",
    "    Instead of making yet another regression problem we can find optimal orthogonal transformation using singular value decomposition.\n",
    "    It turns out that optimal transformation W can be expressed via SVD components\n",
    "\n",
    "        X_transpose * Y = U * S * V_transpose\n",
    "        W* = U * V_transpose\n",
    "\n",
    "    :returns: W* : float matrix[emb_dim x emb_dim] as defined in formulae above\n",
    "    \"\"\"\n",
    "    # YOU CODE HERE\n",
    "\n",
    "    # We use SVD to find the optimal orthogonal transformation\n",
    "    U, S, Vt = np.linalg.svd(X_train.T @ Y_train)\n",
    "    W = U @ Vt\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = learn_transform(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('апрель', 0.8237906694412231),\n",
       " ('сентябрь', 0.8049710988998413),\n",
       " ('март', 0.802565336227417),\n",
       " ('июнь', 0.8021842837333679),\n",
       " ('октябрь', 0.8001735806465149),\n",
       " ('ноябрь', 0.793448269367218),\n",
       " ('февраль', 0.7914119958877563),\n",
       " ('июль', 0.790810763835907),\n",
       " ('август', 0.7891013622283936),\n",
       " ('декабрь', 0.7686371803283691)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_emb.most_similar([np.matmul(uk_emb[\"серпень\"], W)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_top1 = precision(uk_ru_test, np.matmul(X_test, W))\n",
    "precision_top5 = precision(uk_ru_test, np.matmul(X_test, W), 5)\n",
    "\n",
    "assert precision_top1 >= 0.653, \"Achived precision: %f\" % precision_top1 + \" not reaching the threshold of 0.653\"\n",
    "assert precision_top5 >= 0.824, \"Achived precision: %f\" % precision_top5 + \" not reaching the threshold of 0.824\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UK-RU Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to make simple word-based translator: for earch word in source language in shared embedding space we find the nearest in target language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fairy_tale.txt\", \"r\") as inpf:\n",
    "    uk_sentences = [line.rstrip().lower() for line in inpf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    :args:\n",
    "        sentence - sentence in Ukrainian (str)\n",
    "    :returns:\n",
    "        translation - sentence in Russian (str)\n",
    "\n",
    "    * find ukrainian embedding for each word in sentence\n",
    "    * transform ukrainian embedding vector\n",
    "    * find nearest russian word and replace\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # We split the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        # We find the embeddings for each word\n",
    "        try:\n",
    "            embeddings.append(uk_emb[word])\n",
    "        except KeyError:\n",
    "            # If the word is not in the vocabulary\n",
    "            # we append a zero vector\n",
    "            # since we don't know if there is an\n",
    "            # embedding for unknown words (tried unk, UNK, <unk> and <UNK>, don't work)\n",
    "            embeddings.append(np.zeros(uk_emb.vector_size))\n",
    "\n",
    "    # We transform the embeddings using W\n",
    "    transformed_embeddings = np.matmul(embeddings, W)\n",
    "\n",
    "    # We find the nearest russian word for each transformed embedding\n",
    "    nearest_words = [ru_emb.most_similar([embedding], topn=1)[0][0] for embedding in transformed_embeddings]\n",
    "\n",
    "    # We join the words into a sentence\n",
    "    translation = \" \".join(nearest_words)\n",
    "\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert translate(\".\") == \".\"\n",
    "assert translate(\"1 , 3\") == \"1 , 3\"\n",
    "assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: лисичка - сестричка і вовк - панібрат\n",
      "dst: лисичка – сестричка и волк – ,\n",
      "\n",
      "src: як була собі лисичка та зробила хатку, та й живе. а це приходять холоди. от лисичка замерзла та й побігла в село вогню добувать, щоб витопити. прибігає до одної баби та й каже:\n",
      "dst: как была себе лисичка и сделала , и и , а оно приходят , из лисичка замерзла и и побежала во село огня , чтобы , прибегает к одной бабы и и ,\n",
      "\n",
      "src: — здорові були, бабусю! з неділею... позичте мені огню, я вам одслужу.\n",
      "dst: — здоровые , , со , , мне , мной тебе ,\n",
      "\n",
      "src: — добре, — каже, — лисичко - сестричко. сідай погрійся трохи, поки я пиріжечки повибираю з печі!\n",
      "dst: — , — , — , – , садись , , пока мной пирожки , со ,\n",
      "\n",
      "src: а баба макові пиріжки пекла. от баба вибирає пиріжки та на столі кладе, щоб прохололи; а лисичка підгляділа та за пиріг, та з хати... виїла мачок із середини, а туди напхала сміттячка, стулила та й біжить.\n",
      "dst: а бабка маковые пирожки , из бабка выбирает пирожки и по столе , чтобы , а лисичка , и за , и со , , , со , а туда , , , и и ,\n",
      "\n",
      "src: от біжить, а хлопці товар женуть до води.\n",
      "dst: из , а парни товар гонят к воды\n",
      "\n",
      "src: — здорові були, хлопці!\n",
      "dst: — здоровые , ,\n",
      "\n",
      "src: — здорова, лисичко - сестричко!\n",
      "dst: — , , – ,\n",
      "\n",
      "src: — проміняйте мені бичка - третячка на маковий пиріжок!\n",
      "dst: — , мне бычка – , по маковый ,\n",
      "\n",
      "src: — добре, — кажуть.\n",
      "dst: — , — ,\n",
      "\n",
      "src: — тільки, — каже, — тепер не їжте, а як я вибіжу з села, то тоді.\n",
      "dst: — , — , — теперь не , а как мной , со , то ,\n",
      "\n",
      "src: от помінялись. лисичка за бичка — та в ліс. а хлопці до пиріжка — а там сміттянко.\n",
      "dst: из , лисичка за бычка — и во сельско а парни к пирожка — а там ,\n",
      "\n",
      "src: от прибігла лисичка до своєї хатки, вирубала дерево, зробила санки, запрягла бичка — іде. аж біжить вовк:\n",
      "dst: из прибежала лисичка к своего , , , сделала , , бычка — , аж бежит ,\n",
      "\n",
      "src: — здорова, лисичко - сестричко!\n",
      "dst: — , , – ,\n",
      "\n",
      "src: — здоров, вовчику - братику!\n",
      "dst: — , , – ,\n",
      "\n",
      "src: — де ти взяла бичка - третячка і санки?\n",
      "dst: — куда ты взяла бычка – , и ,\n",
      "\n",
      "src: — зробила собі!\n",
      "dst: — сделала ,\n",
      "\n",
      "src: — підвези ж, — каже, — мене, лисичко - сестричко!\n",
      "dst: — , , — , — , , – ,\n",
      "\n",
      "src: — е, куди я тебе візьму? ти мені й санки поламаєш!\n",
      "dst: — , куда мной тебя , ты мне и санки ,\n",
      "\n",
      "src: — ні, — каже, — я тільки одну ніжку положу.\n",
      "dst: — , — , — мной только одну ножку ,\n",
      "\n",
      "src: — ну, клади!\n",
      "dst: — , ,\n",
      "\n",
      "src: от од’їхали трохи, вовк і каже:\n",
      "dst: из , , волк и ,\n",
      "\n",
      "src: — положу я, лисичко - сестричко, й другу ніжку!\n",
      "dst: — , , , – , и вторую ,\n",
      "\n",
      "src: — е, вовчику - братику, ти мені й санки поламаєш!\n",
      "dst: — , , – , ты мне и санки ,\n",
      "\n",
      "src: — ні, — каже, — не поламаю!\n",
      "dst: — , — , — не ,\n",
      "\n",
      "src: — ну, клади!\n",
      "dst: — , ,\n",
      "\n",
      "src: вовк і положив. ідуть, їдуть, коли це щось — трісь.\n",
      "dst: волк и , , , когда оно что-то — ,\n",
      "\n",
      "src: — е, вовчику - братику, ти мені вже й санки ламаєш!\n",
      "dst: — , , – , ты мне уже и санки ,\n",
      "\n",
      "src: — ні лисичко - сестричко, то я орішок розкусив.\n",
      "dst: — ни , – , то мной , ,\n",
      "\n",
      "src: — ну, гляди!\n",
      "dst: — , ,\n",
      "\n",
      "src: от їдуть...\n",
      "dst: из ,\n",
      "\n",
      "src: — положу я, лисичко - сестричко, й третю ніжку! — каже вовк.\n",
      "dst: — , , , – , и третью , — говорит ,\n",
      "\n",
      "src: — куди ти положиш? ти мені санки поламаєш! чим я тоді дровець привезу?\n",
      "dst: — куда ты , ты мне санки , Чем мной тогда , ,\n",
      "\n",
      "src: — ні, — каже, — не поламаю.\n",
      "dst: — , — , — не ,\n",
      "\n",
      "src: — ну, клади.\n",
      "dst: — , ,\n",
      "\n",
      "src: вовк положив і третю ногу. коли це — трісь!\n",
      "dst: волк положил и третью , когда оно — ,\n",
      "\n",
      "src: — ой лихо! — каже лисичка. — йди собі геть, вовчику, — ти мені зовсім санки поламаєш.\n",
      "dst: — ой , — говорит , — иди себе , , — ты мне совсем санки ,\n",
      "\n",
      "src: — ні, то я орішок розкусив!\n",
      "dst: — , то мной , ,\n",
      "\n",
      "src: — дай же й мені!\n",
      "dst: — дай то и ,\n",
      "\n",
      "src: — нема, — каже, — останній!\n",
      "dst: — , — , — ,\n",
      "\n",
      "src: їдуть собі та й їдуть; вовчик і каже:\n",
      "dst: едут себе и и , , и ,\n",
      "\n",
      "src: — сяду я зовсім, лисичко!\n",
      "dst: — сяду мной , ,\n",
      "\n",
      "src: — куди ти сядеш, вовчику - братику? і санки розламаєш!\n",
      "dst: — куда ты , , – , и санки ,\n",
      "\n",
      "src: — я помаленьку, — каже.\n",
      "dst: — мной , — ,\n",
      "\n",
      "src: — ну, гляди!\n",
      "dst: — , ,\n",
      "\n",
      "src: от вовчик тільки що сів, а санки так і розпались... лисичка тоді давай його лаять! лаяла - лаяла та й каже:\n",
      "dst: из , только что , а санки так и , лисичка тогда давай его , ругала – ругала и и ,\n",
      "\n",
      "src: — піди ж, сякий - такий сину, дровець нарубай і на санки вирубай, і приволочи!\n",
      "dst: — пойди , , – такой , , , и по санки , и ,\n",
      "\n",
      "src: — як же я, — каже вовчик, — вирубаю, коли я не знаю, якого дерева?\n",
      "dst: — как то , — говорит , — , когда мной не , которого ,\n",
      "\n",
      "src: — е, — сякий - такий сину! як санчата ламать, так і знав, а дровець вирубать, то й ні!\n",
      "dst: — , — , – такой , как санки , так и , а , , то и ,\n",
      "\n",
      "src: коришувала його, коришувала...\n",
      "dst: , , ,\n",
      "\n",
      "src: — як увійдеш, — каже, — в ліс, то кажи: « рубайся, дерево, й пряме, й криве! рубайся, дерево, й пряме, й криве! »\n",
      "dst: — как , — , — во , то , « , , и , и , , , и , и , »\n",
      "\n",
      "src: вовк і пішов.\n",
      "dst: волк и ,\n",
      "\n",
      "src: от приходить в ліс та й каже:\n",
      "dst: из приходит во лес и и ,\n",
      "\n",
      "src: — рубайся, дерево, криве й криве!\n",
      "dst: — , , кривое и ,\n",
      "\n",
      "src: дерево й нарубалось. таке корячкувате, що й на палицю не вибереш — не то на полозок! приносить вовчик до лисички те дерево. вона як подивилась, давай його знов батькувати.\n",
      "dst: дерево и , такое , что и по палку не выберешь — не то по , приносит , к лисички что , она как , давай его снова ,\n",
      "\n",
      "src: — ти, — каже, — сякий - такий сину, не так казав, як я тобі веліла!\n",
      "dst: — , — , — , – такой , не так , как мной тебе ,\n",
      "\n",
      "src: — ні, лисичко - сестричко, я, — каже, — стояв та все казав: « рубайся, дерево, криве й криве! »\n",
      "dst: — , , – , , — , — стоял и всё , « , , кривое и , »\n",
      "\n",
      "src: — е, бісів сину, й того недотепний! ну, сиди ж ти тут, — я сама піду нарубаю.\n",
      "dst: — , бесов , и того , , сиди же ты , — мной сама пойду ,\n",
      "\n",
      "src: та й пішла.\n",
      "dst: и и ,\n",
      "\n",
      "src: от сидить вовк сам собі — так їсти хочеться! він думав - думав: « давай, — каже, — з’їм бичка та й утечу! » от взяв проїв дірку у бичка, із середини все виїв, а туди горобців напустив і соломою заткнув, а сам — драла... приходить лисичка, зробила санчата, сіла...\n",
      "dst: из сидит волк сам себе — так кушать , он думал – , « , — , — , бычка и и , » из взял , дыру во , со середины всё , а туда воробьёв напустил и соломой , а сам — , приходит , сделала , ,\n",
      "\n",
      "src: — гей, бичок - третячок!\n",
      "dst: — , бычок – ,\n",
      "\n",
      "src: аж бичок не везе. вона його батогом. як ударила, а віхоть соломи й випав; а горобці — хррр!\n",
      "dst: аж бычок не , она его , как , а , соломы и , а воробьи — ,\n",
      "\n",
      "src: — а, сякий - такий вовчик! постій же, — каже, — я тобі згадаю!\n",
      "dst: — , , – такой , постой , — , — мной тебе ,\n",
      "\n",
      "src: та й пішла...\n",
      "dst: и и ,\n",
      "\n",
      "src: лягла на шляху та й лежить. ідуть чумаки з рибою; вона притаїлась, мов нежива. чумаки дивляться — аж лисиця:\n",
      "dst: легла по пути и и , идут казаки со , она , языки , казаки смотрят — аж ,\n",
      "\n",
      "src: — візьмім, — кажуть, — бра, та продамо — буде за що хоч погрітися!\n",
      "dst: — , — , — , и продадим — будет за что хоть ,\n",
      "\n",
      "src: скинули її на останній віз та й поїхали. ідуть та й їдуть. а лисичка - сестричка бачить, що вони не дивляться, та все кида по рибці на дорогу, все кида... от як накидала вже багато, тоді нишком і сама злізла. чумаки ж поїхали собі далі, а вона позбирала рибку, сіла та й їсть.\n",
      "dst: сбросили ей по последний визы и и , идут и и , а лисичка – сестричка , что они не , и всё , по рыбки по , всё , из как навязала уже , тогда втихаря и сама , казаки же поехали себе , а она , , присела и и ,\n",
      "\n",
      "src: коли це біжить вовчик:\n",
      "dst: когда оно бежит ,\n",
      "\n",
      "src: — здорова була, лисичко - сестричко!\n",
      "dst: — здоровая , , – ,\n",
      "\n",
      "src: — здоров, вовчику - братику!\n",
      "dst: — , , – ,\n",
      "\n",
      "src: — що ти робиш, лисичко - сестричко?\n",
      "dst: — что ты , , – ,\n",
      "\n",
      "src: — рибу, — каже, — їм.\n",
      "dst: — , — , — ,\n",
      "\n",
      "src: — дай же й мені!\n",
      "dst: — дай то и ,\n",
      "\n",
      "src: — піди собі налови.\n",
      "dst: — пойди себе ,\n",
      "\n",
      "src: — так як же я наловлю, коли я не вмію?\n",
      "dst: — так как то мной , когда мной не ,\n",
      "\n",
      "src: — ну, як знаєш, а я не дам і кісточки!\n",
      "dst: — , как , а мной не дам и ,\n",
      "\n",
      "src: — так хоч навчи мене, як наловить.\n",
      "dst: — так хоть научить , как ,\n",
      "\n",
      "src: а лисичка й дума: « постій же! ти мого бичка - третячка з’їв — я тепер тобі оддячу! »\n",
      "dst: а лисичка и , « постой , ты моего бычка – , , — мной теперь тебе , »\n",
      "\n",
      "src: — а так, — каже, — піди до ополонки, устроми в ополонку хвіст та потихеньку води ним і приказуй: « ловись, рибко, мала й велика! ловись, рибко, мала й велика! » то вона й наловиться.\n",
      "dst: — а , — , — пойди к , , во прорубь хвост и потихоньку воды ним и , « , , имела и , , , имела и , » то она и ,\n",
      "\n",
      "src: — ну, спасибі за науку! — каже вовчик.\n",
      "dst: — , спасибо за , — говорит ,\n",
      "\n",
      "src: от прибігає вовчик до ополонки, устромив в ополонку хвіст:\n",
      "dst: из прибегает , к , , во прорубь ,\n",
      "\n",
      "src: — ловись, — каже, — рибко, мала й велика!\n",
      "dst: — , — , — , имела и ,\n",
      "\n",
      "src: а лисичка з очерету:\n",
      "dst: а лисичка со ,\n",
      "\n",
      "src: — мерзни, мерзни, вовчий хвіст!\n",
      "dst: — , , волчий ,\n",
      "\n",
      "src: а мороз надворі такий, що аж шкварчить! вовчик хвостиком усе водить та:\n",
      "dst: а мороз дворе , что аж , , хвостиком всё водит ,\n",
      "\n",
      "src: — ловись, рибко, мала й велика!\n",
      "dst: — , , имела и ,\n",
      "\n",
      "src: а лисичка:\n",
      "dst: а ,\n",
      "\n",
      "src: — мерзни, мерзни, вовчий хвіст!\n",
      "dst: — , , волчий ,\n",
      "\n",
      "src: поки ловив вовчик рибу, поки хвіст так і прикипів в ополонці! тоді лисичка в село:\n",
      "dst: пока ловил , , пока хвост так и пугаясь во , тогда лисичка во ,\n",
      "\n",
      "src: — ідіть, люди, вовка бить!\n",
      "dst: — , , волка ,\n",
      "\n",
      "src: люди як вискочать — з кочергами, з рогачами, із сокирами: вбили того вовка і пропав бідний! а лисичка й досі живе у своїй хатці.\n",
      "dst: люди как , — со , со , со , убили того волка и пропал , а лисичка и сих живет во своей ,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in uk_sentences:\n",
    "    print(\"src: {}\\ndst: {}\\n\".format(sentence, translate(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so bad, right? We can easily improve translation using language model and not one but several nearest neighbours in shared embedding space. But next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Would you like to learn more?\n",
    "\n",
    "### Articles:\n",
    "* [Exploiting Similarities among Languages for Machine Translation](https://arxiv.org/pdf/1309.4168)  - entry point for multilingual embedding studies by Tomas Mikolov (the author of W2V)\n",
    "* [Offline bilingual word vectors, orthogonal transformations and the inverted softmax](https://arxiv.org/pdf/1702.03859) - orthogonal transform for unsupervised MT\n",
    "* [Word Translation Without Parallel Data](https://arxiv.org/pdf/1710.04087)\n",
    "* [Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion](https://arxiv.org/pdf/1804.07745)\n",
    "* [Unsupervised Alignment of Embeddings with Wasserstein Procrustes](https://arxiv.org/pdf/1805.11222)\n",
    "\n",
    "### Repos (with ready-to-use multilingual embeddings):\n",
    "* https://github.com/facebookresearch/MUSE\n",
    "\n",
    "* https://github.com/Babylonpartners/fastText_multilingual -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
