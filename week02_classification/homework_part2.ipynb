{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary prediction, episode II: make it actually work (4 points)\n",
    "\n",
    "Your main task is to use some of the tricks you've learned on the network and analyze if you can improve __validation MAE__. Try __at least 3 options__ from the list below for a passing grade. Write a short report about what you have tried. More ideas = more bonus points. \n",
    "\n",
    "__Please be serious:__ \" plot learning curves in MAE/epoch, compare models based on optimal performance, test one change at a time. You know the drill :)\n",
    "\n",
    "You can use either __pytorch__ or __tensorflow__ or any other framework (e.g. pure __keras__). Feel free to adapt the seminar code for your needs. For tensorflow version, consider `seminar_tf2.ipynb` as a starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('comments.tsv', sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total comments: {}'.format(len(data)))\n",
    "texts = data['comment_text'].values\n",
    "target = data['should_ban'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts_train, texts_test, target_train, target_test = train_test_split(texts, target, test_size=0.5, random_state=42)\n",
    "print('Train size: {}'.format(len(texts_train)))\n",
    "print('Test size: {}'.format(len(texts_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenize_func = lambda x: ' '.join(tokenizer.tokenize(x.lower()))\n",
    "tokenize_func('Hello, world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_func_vect = np.vectorize(tokenize_func)\n",
    "print('Before tokenization:')\n",
    "print(texts_train[:3])\n",
    "print()\n",
    "texts_train = tokenize_func_vect(texts_train)\n",
    "texts_test = tokenize_func_vect(texts_test)\n",
    "print('After tokenization:')\n",
    "print(texts_train[:3])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader \n",
    "embeddings = gensim.downloader.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_sum(text, print_missing_words=False):\n",
    "    tokens = text.split()\n",
    "    embeddings_sum = np.zeros(embeddings.vectors.shape[1])\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            embeddings_sum += embeddings.get_vector(token)\n",
    "        except KeyError:\n",
    "            if print_missing_words:\n",
    "                print(f'Word \"{token}\" not found in vocabulary')\n",
    "            pass\n",
    "    return embeddings_sum\n",
    "\n",
    "embedding_sum(tokenize_func('Hello, world!'), True).shape\n",
    "\n",
    "def embedding_stack(text, sentence_len=32, print_missing_words=False):\n",
    "    tokens = text.split()\n",
    "    embeddings_stack = np.zeros((embeddings.vectors.shape[1], sentence_len))\n",
    "    \n",
    "    if len(tokens) < sentence_len:\n",
    "        diff = sentence_len - len(tokens)\n",
    "        tokens = tokens + ['PAD'] * diff\n",
    "    \n",
    "    for i, token in enumerate(tokens[:sentence_len]):\n",
    "        try:\n",
    "            embeddings_stack[:, i] = embeddings.get_vector(token)\n",
    "        except KeyError:\n",
    "            embeddings_stack[:, i] = embeddings.get_vector('UNK')\n",
    "            if print_missing_words:\n",
    "                print(f'Word \"{token}\" not found in vocabulary')\n",
    "            pass\n",
    "    return embeddings_stack\n",
    "\n",
    "embedding_stack(tokenize_func('Hello, world!'), sentence_len=5, print_missing_words=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sentence_len = int(np.mean([len(text.split()) for text in texts_train]))\n",
    "\n",
    "print('Before embedding:')\n",
    "print(texts_train[:3])\n",
    "print()\n",
    "texts_train = np.array([embedding_stack(text, sentence_len=avg_sentence_len) for text in texts_train])\n",
    "texts_test = np.array([embedding_stack(text, sentence_len=avg_sentence_len) for text in texts_test])\n",
    "print('After embedding:')\n",
    "print(texts_train[:3].shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs Architecture\n",
    "\n",
    "We will test the following architectures:\n",
    "\n",
    "1. Simple CNN with one convolutional layer and one fully-connected layer\n",
    "2. CNN with two convolutional layers each with a max-pooling layer and one fully-connected layer\n",
    "3. Parallel CNN with two convolutional layers each with a max-pooling layer, one concatenation and one fully-connected layer\n",
    "\n",
    "*Sources:*\n",
    "- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "- https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def to_tensors(data):\n",
    "    return torch.tensor(data, dtype=torch.float32, device=device)\n",
    "\n",
    "def make_batches(data, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = len(data)\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield to_tensors(data[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X, y):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy for a batch of X and y, using model.\n",
    "    Since the model outputs a single value with sigmoid output, we need to round it to get the predicted class.\n",
    "    @param model: torch.nn.Module\n",
    "    @param X: torch.Tensor or numpy.ndarray\n",
    "    @param y: torch.Tensor or numpy.ndarray\n",
    "    @return: float\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = to_tensors(X)\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = to_tensors(y)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        output = model(X)\n",
    "        if len(output.shape) > 1:\n",
    "            output = output.squeeze()\n",
    "        y_pred = torch.round(output)\n",
    "        return (y_pred == y).float().mean().item()\n",
    "\n",
    "def train(model, optimizer, criterion, data, batch_size=32, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a model using the given optimizer and criterion,\n",
    "    using the given data, for the given number of epochs in batches of batch_size.\n",
    "    @param model: torch.nn.Module\n",
    "    @param optimizer: torch.optim.Optimizer\n",
    "    @param criterion: torch.nn.Module\n",
    "    @param data: tuple - should be [X_train, X_test, y_train, y_test]\n",
    "    @param batch_size: int\n",
    "    @param epochs: int\n",
    "    @return: dict - with keys \"loss\", \"accuracy\", \"test_loss\", \"test_accuracy\"\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = data\n",
    "    assert X_train.shape[0] == y_train.shape[0], \"X_train and y_train must have the same number of rows, keep in mind data should be [X_train, X_test, y_train, y_test]\"\n",
    "    assert X_test.shape[0] == y_test.shape[0], \"X_test and y_test must have the same number of rows, keep in mind data should be [X_train, X_test, y_train, y_test]\"\n",
    "    assert X_train.shape[1] == X_test.shape[1], \"X_train and X_test must have the same number of columns, keep in mind data should be [X_train, X_test, y_train, y_test]\"\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": [],\n",
    "        \"accuracy\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_accuracy\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Train\n",
    "        for X_batch, y_batch in zip(make_batches(X_train, batch_size), make_batches(y_train, batch_size)):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= len(data) / batch_size\n",
    "        metrics[\"loss\"].append(epoch_loss)\n",
    "        metrics[\"accuracy\"].append(accuracy(model, X_train, y_train))\n",
    "        \n",
    "        # Test\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in zip(make_batches(X_test), make_batches(y_test)):\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        epoch_loss /= len(data) / batch_size\n",
    "        metrics[\"test_loss\"].append(epoch_loss)\n",
    "        metrics[\"test_accuracy\"].append(accuracy(model, X_test, y_test))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def plot_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Generates plots for a metrics dictionary with the results of training.\n",
    "    @param metrics: dict - should have keys \"loss\", \"accuracy\", \"test_loss\", \"test_accuracy\"\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Loss')\n",
    "    l = metrics.get('loss', [])\n",
    "    tl = metrics.get('test_loss', [])\n",
    "    if len(l) > 0 or len(tl) > 0: #at least one of them has values\n",
    "        plt.plot(l, label='Train loss')\n",
    "        plt.plot(tl, label='Test loss')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Accuracy')\n",
    "    a = metrics.get('accuracy', [])\n",
    "    ta = metrics.get('test_accuracy', [])\n",
    "    if len(a) > 0 or len(ta) > 0: #at least one of them has values\n",
    "        plt.plot(a, label='Train accuracy')\n",
    "        plt.plot(ta, label='Test accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "def plot_auc(model, X, y, model_name='Model'):\n",
    "    \"\"\"\n",
    "    Calculates the AUC for a batch of X and y, using model.\n",
    "    @param model: torch.nn.Module\n",
    "    @param X: torch.Tensor or numpy.ndarray\n",
    "    @param y: torch.Tensor or numpy.ndarray\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = to_tensors(X)\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = to_tensors(y)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        output = model(X)\n",
    "        if len(output.shape) > 1:\n",
    "            output = output.squeeze()\n",
    "        y_pred = output.cpu().numpy()\n",
    "        y_true = y.cpu().numpy()\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([-0.05, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple CNN with one convolutional layer and one fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_size, sentence_len, hidden_size, filter_size=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, hidden_size, filter_size, padding='same')\n",
    "        self.fc1 = nn.Linear(hidden_size*sentence_len, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "simple_cnn = SimpleCNN(texts_train.shape[1], texts_train.shape[2], 100, 9).to(device)\n",
    "optimizer = torch.optim.Adam(simple_cnn.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "metrics = train(simple_cnn, optimizer, criterion, [texts_train, texts_test, target_train, target_test], batch_size=32, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN with two convolutional layers each with a max-pooling layer and one fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, sentence_len, hidden_size, filter_size=3):\n",
    "        super().__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_size)\n",
    "        self.conv1 = nn.Conv1d(input_size, hidden_size, filter_size, padding='same')\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        sentence_len = sentence_len // 2 # because of maxpool\n",
    "        filter_size = max(3, filter_size // 2) # decreasing filter size\n",
    "        filter_size = filter_size + 1 if filter_size % 2 == 0 else filter_size # making filter size odd (better for padding='same')\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, filter_size, padding='same')\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        sentence_len = sentence_len // 2 # because of maxpool\n",
    "        self.fc1 = nn.Linear(hidden_size*sentence_len, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout1d(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        \n",
    "        x = F.dropout1d(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "cnn = CNN(texts_train.shape[1], texts_train.shape[2], 100, 9).to(device)\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "metrics = train(cnn, optimizer, criterion, [texts_train, texts_test, target_train, target_test], batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel CNN with two convolutional layers each with a max-pooling layer, one concatenation and one fully-connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parallellCNN(nn.Module):\n",
    "    def __init__(self, input_size, sentence_len, hidden_size, filter_size=3):\n",
    "        super().__init__()\n",
    "        # First layers of both branches\n",
    "        self.batch_norm_1 = nn.BatchNorm1d(input_size)\n",
    "        self.batch_norm_2 = nn.BatchNorm1d(input_size)\n",
    "        self.conv1_1 = nn.Conv1d(input_size, hidden_size, filter_size, padding='same')\n",
    "        self.pool1_1 = nn.MaxPool1d(2)\n",
    "        self.conv1_2 = nn.Conv1d(input_size, hidden_size, filter_size, padding='same')\n",
    "        self.pool1_2 = nn.MaxPool1d(2)\n",
    "        sentence_len = sentence_len // 2 # because of maxpool\n",
    "        filter_size = max(3, filter_size // 2) # decreasing filter size\n",
    "        filter_size = filter_size + 1 if filter_size % 2 == 0 else filter_size # making filter size odd (better for padding='same')\n",
    "\n",
    "        # Second layers of both branches\n",
    "        self.conv2_1 = nn.Conv1d(hidden_size, hidden_size, filter_size, padding='same')\n",
    "        self.pool2_1 = nn.MaxPool1d(2)\n",
    "        self.conv2_2 = nn.Conv1d(hidden_size, hidden_size, filter_size, padding='same')\n",
    "        self.pool2_2 = nn.MaxPool1d(2)\n",
    "        sentence_len = sentence_len // 2 # because of maxpool\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(hidden_size*sentence_len*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout1d(x)\n",
    "        x1 = self.batch_norm_1(x)\n",
    "        x2 = self.batch_norm_2(x)\n",
    "        x1 = F.dropout(x1)\n",
    "        x2 = F.dropout(x2)\n",
    "        x1 = self.pool1_1(F.relu(self.conv1_1(x1)))\n",
    "        x2 = self.pool1_2(F.relu(self.conv1_2(x2)))\n",
    "        x1 = F.dropout(x1)\n",
    "        x2 = F.dropout(x2)\n",
    "        x1 = self.pool2_1(F.relu(self.conv2_1(x1)))\n",
    "        x2 = self.pool2_2(F.relu(self.conv2_2(x2)))\n",
    "        x1 = torch.flatten(x1, 1)\n",
    "        x2 = torch.flatten(x2, 1)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x.squeeze()\n",
    "\n",
    "par_cnn = parallellCNN(texts_train.shape[1], texts_train.shape[2], 100, 9).to(device)\n",
    "optimizer = torch.optim.Adam(par_cnn.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "metrics = train(par_cnn, optimizer, criterion, [texts_train, texts_test, target_train, target_test], batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_auc(simple_cnn, texts_test, target_test, 'Simple CNN')\n",
    "plot_auc(cnn, texts_test, target_test, 'CNN')\n",
    "plot_auc(par_cnn, texts_test, target_test, 'Parallel CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Architecture\n",
    "\n",
    "We will test the following architechtures:\n",
    "\n",
    "1. Simple RNN with one fully-connected layer\n",
    "2. RNN with two fully-connected layers\n",
    "3. Parallel RNN one left-to-right and one right-to-left (Bidirectional RNN) with a final concatenation and one fully-connected layer\n",
    "4. Parallel LSTM one left-to-right and one right-to-left (Bidirectional LSTM) with a final concatenation and one fully-connected layer\n",
    "5. Parallel GRU one left-to-right and one right-to-left (Bidirectional GRu) with a final concatenation and one fully-connected layer\n",
    "\n",
    "*Sources:*\n",
    "- https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "- https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <CODE-HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short report\n",
    "\n",
    "Please tell us what you did and how did it work.\n",
    "\n",
    "`<YOUR_TEXT_HERE>`, i guess..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended options\n",
    "\n",
    "#### A) CNN architecture\n",
    "\n",
    "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
    "* Dropout. Nuff said.\n",
    "* Batch Norm. This time it's `nn.BatchNorm*`/`L.BatchNormalization`\n",
    "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n",
    "* More layers, more neurons, ya know...\n",
    "\n",
    "\n",
    "#### B) Play with pooling\n",
    "\n",
    "There's more than one way to perform pooling:\n",
    "* Max over time (independently for each feature)\n",
    "* Average over time (excluding PAD)\n",
    "* Softmax-pooling:\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
    "\n",
    "* Attentive pooling\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n",
    "\n",
    ", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
    "and $NN_{attn}$ is a dense layer.\n",
    "\n",
    "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$ (aka multi-headed attention).\n",
    "\n",
    "The catch is that keras layers do not inlude those toys. You will have to [write your own keras layer](https://keras.io/layers/writing-your-own-keras-layers/). Or use pure tensorflow, it might even be easier :)\n",
    "\n",
    "#### C) Fun with words\n",
    "\n",
    "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
    "\n",
    "* Use a pre-trained embeddings from `gensim.downloader.load`. See last lecture.\n",
    "* Start with pre-trained embeddings, then fine-tune them with gradient descent. You may or may not download pre-trained embeddings from [here](http://nlp.stanford.edu/data/glove.6B.zip) and follow this [manual](https://keras.io/examples/nlp/pretrained_word_embeddings/) to initialize your Keras embedding layer with downloaded weights.\n",
    "* Use the same embedding matrix in title and desc vectorizer\n",
    "\n",
    "\n",
    "#### D) Going recurrent\n",
    "\n",
    "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
    "\n",
    "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
    "* Since you know all the text in advance, use bidirectional RNN\n",
    "  * Run one LSTM from left to right\n",
    "  * Run another in parallel from right to left \n",
    "  * Concatenate their output sequences along unit axis (dim=-1)\n",
    "\n",
    "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
    "\n",
    "\n",
    "#### E) Optimizing seriously\n",
    "\n",
    "* You don't necessarily need 100 epochs. Use early stopping. If you've never done this before, take a look at [early stopping callback(keras)](https://keras.io/callbacks/#earlystopping) or in [pytorch(lightning)](https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html).\n",
    "  * In short, train until you notice that validation\n",
    "  * Maintain the best-on-validation snapshot via `model.save(file_name)`\n",
    "  * Plotting learning curves is usually a good idea\n",
    "  \n",
    "Good luck! And may the force be with you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
